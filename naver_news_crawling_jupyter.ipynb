{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10페이지 넘어가는 경우 해결\n",
    "# 저장 구조 바꾸기, 제목, 언론사, 날짜 전부 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import re\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = \"./chromedriver\"\n",
    "puzzle_url = get_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class driver(webdriver.Chrome):\n",
    "    \n",
    "    pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "driv =driver(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "driv.get(puzzle_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = driver.current_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_now = driver.page_source\n",
    "page_now = BeautifulSoup(page_now, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "list_tmp = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    page_url = tmp + '&page=' + str(i+1)\n",
    "    driver.get(page_url)\n",
    "    \n",
    "    today_html = driver.page_source\n",
    "    listup = BeautifulSoup(today_html, 'lxml')\n",
    "\n",
    "    news_list = listup.find_all('a', {'class' : 'nclicks(cnt_papaerart)'})\n",
    "    news_list += listup.find_all('a', {'class' : 'nclicks(cnt_papaerart3)'})\n",
    "    news_list += listup.find_all('a', {'class' : 'nclicks(cnt_papaerart4)'})\n",
    "    news_list += listup.find_all('a', {'class' : 'nclicks(cnt_flashart)'})\n",
    "    \n",
    "    if(list_tmp[0] == news_list[0]):\n",
    "        break\n",
    "    list_tmp = news_list[0]\n",
    "\n",
    "    \n",
    "    for index in range(len(news_list)):\n",
    "    \n",
    "        count += 1\n",
    "        addr = news_list[index]['href']\n",
    "        driver.get(addr)\n",
    "\n",
    "        # 스포츠 뉴스와 연예 뉴스는 제외 (형식도 다르고 목적과 맞지 않음.)\n",
    "        check = driver.current_url\n",
    "        if ('sports' in check) or ('entertain' in check):\n",
    "            continue\n",
    "\n",
    "        html = driver.page_source\n",
    "\n",
    "        # 타이틀, 분류, 날짜, 언론사, 내용, 댓글 \n",
    "        dom = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        category_raw = dom.find('em', {'class' : 'guide_categorization_item'})\n",
    "        category = category_raw.text\n",
    "\n",
    "        title_raw = dom.find_all('h3', {'id' : 'articleTitle'})\n",
    "        title = [title.text for title in title_raw]\n",
    "        title = str(title[0])\n",
    "\n",
    "        # 저장시 문제 안생기게\n",
    "        title = re.sub(\"\\\"\",'',title)\n",
    "        title = re.sub(\"\\'\",'',title)\n",
    "        title = re.sub(\"‘\",'',title)\n",
    "        title = re.sub(\"’\",'',title)\n",
    "        title = re.sub(\" \",'',title)\n",
    "        title = re.sub(\"\\?\",'',title)\n",
    "        title = re.sub(\":\",'',title)\n",
    "        title = re.sub(\"\\/\",'',title)\n",
    "\n",
    "        date_raw = dom.find_all('span', {'class' : 't11'})\n",
    "        date = date_raw[0].text.split()[0]\n",
    "\n",
    "        press_raw = dom.find('div', {'class' : 'press_logo'})\n",
    "        press = press_raw.select('a')[0].find('img')['title']\n",
    "\n",
    "        category_raw = dom.find('em', {'class' : 'guide_categorization_item'})\n",
    "        category = category_raw.text\n",
    "\n",
    "        contents_raw = dom.find('div', {'id' : 'articleBodyContents'})\n",
    "        contents = contents_raw.text\n",
    "\n",
    "        # 네이버 뉴스에는 아래와 같은 주석이 항상 있음. 이 주석을 제거하기 위한 코드\n",
    "        # \\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\\n\\n \n",
    "        clean_index = contents.index('removeCallback') + 23\n",
    "        contents = contents[clean_index :]\n",
    "\n",
    "\n",
    "        # 기사 포맷이 거의 항상 아래와 같음. 필요 없는 정보를 제거하기 위한 코드\n",
    "        # [ⓒ한겨레신문 : 무단전재 및 재배포 금지]\n",
    "        if '재배포' in contents:\n",
    "            reporter_index = contents.index('재배포') - 15\n",
    "            contents = contents[:reporter_index]\n",
    "\n",
    "        time.sleep(0.1)\n",
    "        pages = 0\n",
    "\n",
    "        try:\n",
    "            driver.find_element_by_css_selector(\".u_cbox_in_view_comment\").click()\n",
    "            time.sleep(0.2)\n",
    "        except exceptions.ElementNotInteractableException as e:\n",
    "            pass\n",
    "        except exceptions.NoSuchElementException as e:\n",
    "            try:\n",
    "                new_addr = dom.find_all('div', {'class' : 'simplecmt_links'})\n",
    "                new_addr = new_addr[0].select('a')[0]['href']\n",
    "                driver.get(new_addr)\n",
    "                time.sleep(0.1)\n",
    "            except:\n",
    "                pass\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            driver.find_element_by_css_selector(\".u_cbox_sort_label\").click()\n",
    "            time.sleep(0.1)\n",
    "        except exceptions.NoSuchElementException as e:\n",
    "            pass\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        try:\n",
    "            while i < 30 :\n",
    "                driver.find_element_by_css_selector(\".u_cbox_btn_more\").click()\n",
    "                time.sleep(0.1)\n",
    "                i+=1\n",
    "\n",
    "        except exceptions.ElementNotVisibleException as e: # 페이지 끝\n",
    "            pass\n",
    "\n",
    "        except Exception as e: # 다른 예외 발생시 확인\n",
    "            print(e)\n",
    "\n",
    "        html = driver.page_source\n",
    "        dom = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        comments_raw = dom.find_all('span', {'class' : 'u_cbox_contents'})\n",
    "        comments = [comment.text for comment in comments_raw]\n",
    "\n",
    "        if (len(comments)<1):\n",
    "            continue\n",
    "\n",
    "        like_comments_raw = dom.find_all('em', {'class' : 'u_cbox_cnt_recomm'})\n",
    "        like_comments = [int(like.text) for like in like_comments_raw]\n",
    "\n",
    "        hate_comments_raw = dom.find_all('em', {'class' : 'u_cbox_cnt_unrecomm'})\n",
    "        hate_comments = [int(hate.text) for hate in hate_comments_raw]\n",
    "\n",
    "        df = pd.DataFrame({'comment' : comments, 'like' : like_comments, 'dont_like' : hate_comments})\n",
    "        df = df.sort_values(by = 'like', ascending = False)\n",
    "\n",
    "        file_name = './'+ category+ '/' + press+'_'+date+'_'+title + '.json'\n",
    "\n",
    "        file_data = OrderedDict()\n",
    "        file_data['contents'] = contents\n",
    "        file_data['comment'] = df['comment'].values.tolist()\n",
    "        file_data['like'] = df['like'].values.tolist()\n",
    "        file_data['dont_like'] = df['dont_like'].values.tolist()\n",
    "\n",
    "        directory = './' + category\n",
    "\n",
    "        if os.path.exists(directory):\n",
    "            with open(file_name, 'w', encoding = 'utf-8') as make_file:\n",
    "                json.dump(file_data, make_file, ensure_ascii=False, indent='\\t')\n",
    "\n",
    "        else:\n",
    "            os.mkdir(directory)\n",
    "            with open(file_name, 'w', encoding = 'utf-8') as make_file:\n",
    "                json.dump(file_data,  make_file,ensure_ascii=False, indent='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of articles: {}'.format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('./정치/한겨레_2019.07.28._넉달째맹탕국회…정치가없다.json', encoding = 'utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    df = pd.DataFrame({'comment' : data['comment'], 'like':data['like'], 'dont_like':data['dont_like']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "data['contents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input():\n",
    "    \n",
    "    press_dict = {'경향신문' : '032' , '국민일보' : '005', '동아일보' : '020', '문화일보' : '021', '서울신문' : '081', '세계일보' : '022', '조선일보' : '023', '중앙일보' : '025', '한겨레' : '028', '한국일보' : '469'}\n",
    "    print('크롤링을 원하는 언론사를 입력.')\n",
    "    print('ex) 경향신문, 국민일보, 동아일보, 문화일보, 서울신문, 세계일보, 조선일보, 중앙일보, 한겨레, 한국일보 // 이외의 언론사는 0 입력')\n",
    "    test_press = input()\n",
    "\n",
    "    if test_press in press_dict.keys():\n",
    "        get_number = press_dict[test_press]\n",
    "        print('원하는 날짜를 입력(yyyymmdd)')\n",
    "        test_date = input()\n",
    "        puzzle_url = 'https://news.naver.com/main/list.nhn?mode=LPOD&mid=sec&oid=' + get_number + '&date=' + test_date\n",
    "        print('{}의 {}날짜 뉴스를 크롤링합니다.'.format(test_press, test_date))\n",
    "        \n",
    "    else:\n",
    "        print('입력한 언론사가 리스트에 없습니다. https://news.naver.com/main/officeList.nhn 에 들어가서 원하는 언론사의 url을 입력해주세요.')\n",
    "        puzzle_url = input()\n",
    "        driver.get(puzzle_url)\n",
    "        input_now = driver.page_source\n",
    "        input_source = BeautifulSoup(input_now, 'lxml')\n",
    "        page_list = input_source.find_all('div', {'class' : 'newsflash_header3'})\n",
    "        press_now = page_list[0].h3.text\n",
    "        print('{} 맞나요? 원하는 날짜를 입력(yyyymmdd).'.format(press_now))\n",
    "        test_date = input()\n",
    "        print('{}의 {}날짜 뉴스를 크롤링합니다.'.format(press_now, test_date))\n",
    "        puzzle_url = puzzle_url+ '&date=' + test_date\n",
    "        \n",
    "    return puzzle_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
