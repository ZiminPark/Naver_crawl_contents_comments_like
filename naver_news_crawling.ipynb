{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import re\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class crawler(webdriver.Chrome):\n",
    "    \n",
    "    def get_input(self):\n",
    "    \n",
    "        press_dict = {'경향신문' : '032' , '국민일보' : '005', '동아일보' : '020', '문화일보' : '021', '서울신문' : '081', \\\n",
    "                      '세계일보' : '022', '조선일보' : '023', '중앙일보' : '025', '한겨레' : '028', '한국일보' : '469'}\n",
    "        print('크롤링을 원하는 언론사를 입력.\\n')\n",
    "        print(\"ex) 경향신문, 국민일보, 동아일보, 문화일보, 서울신문, 세계일보, 조선일보, 중앙일보, 한겨레, 한국일보, \\\n",
    "이외의 언론사는 0 입력\\n\")\n",
    "        test_press = input()\n",
    "\n",
    "        if test_press in press_dict.keys():\n",
    "            crawler.press = test_press\n",
    "            get_number = press_dict[test_press]\n",
    "            print('\\n원하는 날짜를 입력(yyyymmdd)')\n",
    "            test_date = input()\n",
    "            puzzle_url = 'https://news.naver.com/main/list.nhn?mode=LPOD&mid=sec&oid=' + get_number + '&date=' + test_date\n",
    "            print('\\n{}의 {}날짜 뉴스를 크롤링합니다.\\n'.format(test_press, test_date))\n",
    "\n",
    "        else:\n",
    "            print('\\n입력한 언론사가 리스트에 없습니다. https://news.naver.com/main/officeList.nhn 에 들어가서 원하는 언론사의 url을 \\\n",
    "입력해주세요.\\n')\n",
    "            puzzle_url = input()\n",
    "            driver.get(puzzle_url)\n",
    "            input_now = driver.page_source\n",
    "            input_source = BeautifulSoup(input_now, 'lxml')\n",
    "            page_list = input_source.find_all('div', {'class' : 'newsflash_header3'})\n",
    "            press_now = page_list[0].h3.text\n",
    "            crawler.press = press_now\n",
    "            print('\\n{} 맞나요? 원하는 날짜를 입력(yyyymmdd).\\n'.format(press_now))\n",
    "            test_date = input()\n",
    "            print('\\n{}의 {}날짜 뉴스를 크롤링합니다.\\n'.format(press_now, test_date))\n",
    "            puzzle_url = puzzle_url+ '&date=' + test_date\n",
    "\n",
    "        return puzzle_url\n",
    "    \n",
    "    def move_page(self, page_num): # 어떤 날의 여러 페이지 중에 하나로 이동하고 url 을 얻는 method\n",
    "        page_url = puzzle_url + '&page=' + str(page_num)\n",
    "        driver.get(page_url)\n",
    "        return page_url\n",
    "    \n",
    "    def list_up(self, html):\n",
    "        \n",
    "        listup = BeautifulSoup(html, 'lxml')\n",
    "        lists = listup.find_all('a', {'class' : 'nclicks(cnt_papaerart)'})\n",
    "        lists += listup.find_all('a', {'class' : 'nclicks(cnt_papaerart3)'})\n",
    "        lists += listup.find_all('a', {'class' : 'nclicks(cnt_papaerart4)'})\n",
    "        lists += listup.find_all('a', {'class' : 'nclicks(cnt_flashart)'})\n",
    "        \n",
    "        news_list = [article for article in lists if type(article.find('img')) != bs4.element.Tag] # 이미지는 제외\n",
    "        \n",
    "        return news_list\n",
    "    \n",
    "    def break_check(self, news_list, list_tmp): #예시) 14페이지와 15페이지의 뉴스리스트가 같다면 break \n",
    "        \n",
    "        if(list_tmp == news_list[0]): \n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def exclude_sports_ent(self):\n",
    "        check = driver.current_url\n",
    "        if ('sports' in check) or ('entertain' in check):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_data(self, speed = 0.2, num_comments = 700): # 제목, 분류, 날짜, 언론사, 내용, 댓글 수집 \n",
    "        # speed는 댓글 더보기를 누르는 간격, 0.1초로 하면 건너뛰는 경우가 있음. \n",
    "        # num_comments는 크롤링하고 싶은 댓글의 수, 삭제된 댓글 포함.\n",
    "        \n",
    "        html = driver.page_source\n",
    "        dom = BeautifulSoup(html, 'lxml')\n",
    "        current_url = driver.current_url\n",
    "\n",
    "        category_raw = dom.find('em', {'class' : 'guide_categorization_item'}) # 분류\n",
    "        category = category_raw.text\n",
    "\n",
    "        title_raw = dom.find_all('h3', {'id' : 'articleTitle'}) # 기사 제목\n",
    "        title = [title.text for title in title_raw]\n",
    "        title = str(title[0])\n",
    "        original_title = title # 제목 원본\n",
    "\n",
    "        title = re.sub('[^0-9a-zA-Zㄱ-힗]', '', title) # 저장시 문제 안생기게 전처리한 제목\n",
    "\n",
    "        date_raw = dom.find_all('span', {'class' : 't11'}) # 날짜\n",
    "        date = date_raw[0].text.split()[0]\n",
    "\n",
    "        press_raw = dom.find('div', {'class' : 'press_logo'}) #언론사\n",
    "        press = crawler.press\n",
    "\n",
    "        contents_raw = dom.find('div', {'id' : 'articleBodyContents'}) # 뉴스 내용\n",
    "        contents = contents_raw.text\n",
    "\n",
    "        # 네이버 뉴스에는 아래와 같은 주석이 항상 있음. 이 주석을 제거하기 위한 코드\n",
    "        # \\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\\n\\n \n",
    "        clean_index = contents.index('removeCallback') + 23\n",
    "        contents = contents[clean_index :]\n",
    "\n",
    "        # 기사 포맷이 거의 항상 아래와 같음. 필요 없는 정보를 제거하기 위한 코드\n",
    "        # [ⓒ한겨레신문 : 무단전재 및 재배포 금지]\n",
    "        if '재배포' in contents:\n",
    "            reporter_index = contents.index('재배포') - 15\n",
    "            contents = contents[:reporter_index]\n",
    "\n",
    "        time.sleep(speed)\n",
    "        \n",
    "        try:\n",
    "            driver.find_element_by_css_selector(\".u_cbox_in_view_comment\").click() #댓글 보기 누르는 코드\n",
    "            time.sleep(speed)\n",
    "        except exceptions.ElementNotInteractableException as e:\n",
    "            pass\n",
    "        except exceptions.NoSuchElementException as e:\n",
    "            try:\n",
    "                new_addr = dom.find_all('div', {'class' : 'simplecmt_links'})\n",
    "                new_addr = new_addr[0].select('a')[0]['href']\n",
    "                driver.get(new_addr)\n",
    "                time.sleep(speed)\n",
    "            except:\n",
    "                pass\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            driver.find_element_by_css_selector(\".u_cbox_sort_label\").click() #공감순으로 보기 누르는 코드\n",
    "            time.sleep(speed)\n",
    "        except exceptions.NoSuchElementException as e:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            for i in range(num_comments//20):\n",
    "                driver.find_element_by_css_selector(\".u_cbox_btn_more\").click() # 댓글 더보기 누르는 코드\n",
    "                time.sleep(speed)\n",
    "        except exceptions.ElementNotVisibleException as e: #댓글 페이지 끝\n",
    "            pass\n",
    "\n",
    "        except Exception as e: # 다른 예외 발생시 확인\n",
    "            pass\n",
    "\n",
    "        html = driver.page_source # 댓글 크롤링 코드\n",
    "        dom = BeautifulSoup(html, 'lxml')\n",
    "        comments_raw = dom.find_all('span', {'class' : 'u_cbox_contents'})\n",
    "        comments = [comment.text for comment in comments_raw]\n",
    "\n",
    "        like_comments_raw = dom.find_all('em', {'class' : 'u_cbox_cnt_recomm'}) # 공감수\n",
    "        like_comments = [int(like.text) for like in like_comments_raw]\n",
    "\n",
    "        hate_comments_raw = dom.find_all('em', {'class' : 'u_cbox_cnt_unrecomm'}) # 비공감수\n",
    "        hate_comments = [int(hate.text) for hate in hate_comments_raw]\n",
    "        \n",
    "        if (len(comments)<1): #댓글이 없는 경우\n",
    "            comments = []\n",
    "            like_comments = []\n",
    "            hate_comments = []\n",
    "        \n",
    "        data_list = [category, title, original_title, date, press, contents, comments, like_comments, hate_comments, current_url]\n",
    "        \n",
    "        return data_list\n",
    "    \n",
    "    def save_file(self, data_list):\n",
    "        \n",
    "        file_name = './'+ data_list[4] + '/' + data_list[0]+ '_'  + data_list[4] + '_' + data_list[3] +'_'+ data_list[1] + '.json'\n",
    "        file_data = OrderedDict()\n",
    "        \n",
    "        file_data['url'] = data_list[9]\n",
    "        file_data['press'] = data_list[4]\n",
    "        file_data['date'] = data_list[3]\n",
    "        file_data['category'] = data_list[0]\n",
    "        file_data['title'] = data_list[2]\n",
    "        file_data['contents'] = data_list[5]\n",
    "        file_data['comment'] = data_list[6]\n",
    "        file_data['like'] = data_list[7]\n",
    "        file_data['dont_like'] = data_list[8]\n",
    "\n",
    "        directory = './' + data_list[4]\n",
    "\n",
    "        if os.path.exists(directory):\n",
    "            with open(file_name, 'w', encoding = 'utf-8') as make_file:\n",
    "                json.dump(file_data, make_file, ensure_ascii=False, indent='\\t')\n",
    "\n",
    "        else:\n",
    "            os.mkdir(directory)\n",
    "            with open(file_name, 'w', encoding = 'utf-8') as make_file:\n",
    "                json.dump(file_data,  make_file,ensure_ascii=False, indent='\\t')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = \"./chromedriver\"\n",
    "driver = crawler(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤링을 원하는 언론사를 입력.\n",
      "\n",
      "ex) 경향신문, 국민일보, 동아일보, 문화일보, 서울신문, 세계일보, 조선일보, 중앙일보, 한겨레, 한국일보, 이외의 언론사는 0 입력\n",
      "\n",
      "경향신문\n",
      "\n",
      "원하는 날짜를 입력(yyyymmdd)\n",
      "20190808\n",
      "\n",
      "경향신문의 20190808날짜 뉴스를 크롤링합니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "puzzle_url = driver.get_input() # 크롤링하고 싶은 언론사와 날짜를 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956042\n",
      "\"일본, ‘화이트리스트’ 품목 추가지정 안 했다\" 본문과 댓글 13개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956053\n",
      "\"“아베 총리 나빠” [포토뉴스]\" 본문과 댓글 3개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956048\n",
      "\"세상 밖으로 나오는… 은둔형 외톨이의 ‘인생 맛을 배우는 집’\" 본문과 댓글 41개를 크롤링.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Service.__del__ at 0x0000020AA5F93510>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\harder\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 176, in __del__\n",
      "    self.stop()\n",
      "  File \"C:\\harder\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 151, in stop\n",
      "    self.send_remote_shutdown_command()\n",
      "  File \"C:\\harder\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 132, in send_remote_shutdown_command\n",
      "    if not self.is_connectable():\n",
      "  File \"C:\\harder\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 115, in is_connectable\n",
      "    return utils.is_connectable(self.port)\n",
      "  File \"C:\\harder\\lib\\site-packages\\selenium\\webdriver\\common\\utils.py\", line 106, in is_connectable\n",
      "    socket_ = socket.create_connection((host, port), 1)\n",
      "  File \"C:\\harder\\lib\\socket.py\", line 716, in create_connection\n",
      "    sock.connect(sa)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956021\n",
      "\"트럼프 \"한국, 방위비 분담금 증액 동의\"\" 본문과 댓글 409개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956021\n",
      "Error\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956143\n",
      "\"\"강제동원 피해자와 함께 싸우자\"…한일 시민단체, 광복절 행사 동참\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956142\n",
      "\"‘입추’에도 폭염은 기세등등…절기와 실제 날씨가 다른 이유는 [날씨가 왜 이래]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956141\n",
      "\"뇌전증 환자 약 36만명… 수술받는 이들은 대기자의 1%\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956140\n",
      "\"‘아동 성적 대상화’ 베스킨라빈스 광고한 CJ ENM 채널···방심위, 중징계 추진\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956139\n",
      "\"영남대 교수들이 박근혜 전 대통령 ‘그림자 실세’ 최외출 교수 비리 고발한 사연은?\" 본문과 댓글 10개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956138\n",
      "\"한은 금통위 10월 통화정책방향 결정회의, ‘G20’에 16일로 하루 앞당겨\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956137\n",
      "\"황교안 \"문 대통령 벙어리\" 발언, 장애인 비하 '뭇매'\" 본문과 댓글 84개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956136\n",
      "\"스텔라데이지호 침몰원인 규명과 유해수습 촉구 오체투지 [경향포토]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956135\n",
      "\"농협은행, 소재·부품 전문기업에 금리우대 등 3조 대출 지원\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956134\n",
      "\"스텔라데이지호 침몰원인 규명하라 [경향포토]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956133\n",
      "\"‘락의 향연’…펜타포트 락 페스티벌 9일 개막 [인천시]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956132\n",
      "\"엄마부대 주옥순, 또 소녀상 옆에서 정부 규탄 회견···“문 정부가 한·일협정 어겨” 주장\" 본문과 댓글 193개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956131\n",
      "\"제주, 불법체류자 집단숙소에서 30명 무더기 검거\" 본문과 댓글 4개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956130\n",
      "\"이란 난민 김민혁군 아버지 난민 재심사에서 '인도적 체류' 허가 [경향포토]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956129\n",
      "\"이란 출신 김민혁군 아버지 난민 재심서 '불인정' [경향포토]\" 본문과 댓글 2개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956128\n",
      "\"MBC기자회  “취재진 폭행 이영훈 교수, 언론자유에 폭력 행사”\" 본문과 댓글 69개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956127\n",
      "\"기재부 차관 “내년에는 적극적 재정 할 수 밖에 없을 것”\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956126\n",
      "\"523마리 구조·치료…인천야생동물센터 ‘파수꾼’ 역할 톡톡\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956125\n",
      "\"일본 정부에 사과하라는 엄마 부대 [경향포토]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956124\n",
      "\"‘이란 난민’ 김민혁군 아버지 재심사도 난민 불인정···‘인도적 체류’ 결정\" 본문과 댓글 247개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956123\n",
      "\"국민경제자문회의 모두 발언하는 문재인대통령 [경향포토]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956122\n",
      "\"긴급 소집된 국민경제자문회의 [경향포토]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956121\n",
      "\"국민경제자문회의 주재하는 문재인대통령 [경향포토]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956120\n",
      "\"병의원 프로포폴 임의 투약 땐 최장 1년 영업정지\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956119\n",
      "\"“스마트폰으로 밭에 물좀 줘봐”...여행 중에도 자기 밭에 물 주는 시대\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956118\n",
      "\"북미 하노이 정상회담 결렬직후 북한과 중국 무역 급증\" 본문과 댓글 1개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956117\n",
      "\"한미 국방장관 회담 9일 개최···방위비분담금·GSOMIA 등 언급 주목\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956116\n",
      "\"고동진, 일본 수출 규제＂3~4개월은 준비, 계속되면 힘들어. 사장 되고 처음 위기＂\" 본문과 댓글 19개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956115\n",
      "\"청와대 앞에서 구호 외치는  공공연대노조 [경향포토]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956114\n",
      "\"공공연대노조의 청와대 앞 기자회견 [경향포토]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956113\n",
      "\"'선 자세로 대기’ ‘고객화장실 이용 금지’...인권위 “유통업 노동자 건강권 보장해야\"\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956112\n",
      "\"샤이니·엑소·NCT 뭉친 ‘SM 어벤져스’ SuperM, 10월 글로벌 데뷔\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956111\n",
      "\"박지원 “북한 미사일 발사, 트럼프와 합의 있었을 것”\" 본문과 댓글 49개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956110\n",
      "\"경찰간부,아들 통해 불법오락실 영업 조폭에게 단속정보 누설 혐의 기소\" 본문과 댓글 5개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956109\n",
      "\"독일도 사우디도 '화들짝'...미·중 경제전쟁에 \"글로벌 경기침체 온다\" [뉴스 깊이보기]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956108\n",
      "\"'시민과 함께'... 13일 위안부 기림의 날 행사 [성남시]\" 본문과 댓글 1개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956107\n",
      "\"행안부, 새 광화문광장에 또 제동···서울시 “납득 어렵다”\" 본문과 댓글 2개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956106\n",
      "\"화촌·북방면에 ‘유색벼 팜아트 단지’ 조성 [홍천군]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956105\n",
      "\"“폭염으로부터 시민안전 지킨다”…클린로드시스템 가동, 살수차 운영 [춘천시]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956104\n",
      "\"고개숙여 인사하는 윤석열 검찰총장 [경향포토]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956103\n",
      "\"이야기 나누는 황교안 대표  -윤석열 총장 [경향포토]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956102\n",
      "\"고개숙여 답변하는 윤석열 검찰총장 [경향포토]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956101\n",
      "\"자리 앉는 황교안 -윤석열 [경향포토]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956100\n",
      "\"악수하는 황교안 대표 -윤석열 총장 [경향포토]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956099\n",
      "\"윤석열 검찰총장에게 손 잡는 황교안 대표 [경향포토]\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956098\n",
      "\"습한 여름이면 더 아픈 ‘메니에르병’ 궁금증 5가지\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956096\n",
      "\"통신 3사 9일부터 ‘갤럭시노트10’ 사전판매 돌입\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956097\n",
      "\"“희망 노래하며 건강의 꽃 피워갔던 시간, 이제 모두와 나누려 합니다”\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956097\n",
      "Error\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956094\n",
      "\"충북 중학교 여교사가 같은학교 남학생과 성관계\" 본문과 댓글 207개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956093\n",
      "\"동물이 행복해야 인간도 행복...동물복지농장 '급증'\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=032&aid=0002956092\n",
      "\"문 대통령 “日 무역보복，모두가 피해자 되는 승자없는 게임”\" 본문과 댓글 5개를 크롤링.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0 # 크롤링한 기사 수 체크용\n",
    "list_tmp = [0] # 페이지 체크용\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    page_url = driver.move_page(i+1)\n",
    "    today_html = driver.page_source\n",
    "    news_list = driver.list_up(today_html)\n",
    "    \n",
    "    if driver.break_check(news_list,list_tmp): #예시) 14페이지와 15페이지의 뉴스리스트가 같다면 break \n",
    "        break\n",
    "    else:\n",
    "        list_tmp = news_list[0]\n",
    "\n",
    "    for index in range(len(news_list)): ### 몇 페이지 크롤링할 것인지 변수로 받기\n",
    "        try:\n",
    "            count += 1\n",
    "            addr = news_list[index]['href']\n",
    "            driver.get(addr)\n",
    "            # 스포츠 뉴스와 연예 뉴스는 제외 (형식도 다르고 목적과 맞지 않음.)\n",
    "            if driver.exclude_sports_ent():\n",
    "                continue\n",
    "\n",
    "            data_list = driver.get_data(0.2, 700)\n",
    "            print(data_list[9])\n",
    "            print(\"\\\"{}\\\" 본문과 댓글 {}개를 크롤링.\\n\".format(data_list[2], len(data_list[7])))\n",
    "            driver.save_file(data_list) # 데이터 저장\n",
    "            \n",
    "        except:\n",
    "            print(data_list[9])\n",
    "            print(\"Error\\n\")\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of articles: {}'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
