{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import re\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class crawler(webdriver.Chrome):\n",
    "    \n",
    "    def get_input(self):\n",
    "    \n",
    "        press_dict = {'경향신문' : '032' , '국민일보' : '005', '동아일보' : '020', '문화일보' : '021', '서울신문' : '081', \\\n",
    "                      '세계일보' : '022', '조선일보' : '023', '중앙일보' : '025', '한겨레' : '028', '한국일보' : '469'}\n",
    "        print('크롤링을 원하는 언론사를 입력.\\n')\n",
    "        print(\"ex) 경향신문, 국민일보, 동아일보, 문화일보, 서울신문, 세계일보, 조선일보, 중앙일보, 한겨레, 한국일보, \\\n",
    "이외의 언론사는 0 입력\\n\")\n",
    "        input_press = input()\n",
    "\n",
    "        if input_press in press_dict.keys():\n",
    "            self.press = input_press\n",
    "            get_number = press_dict[input_press]\n",
    "            print('\\n원하는 날짜를 입력(yyyymmdd)')\n",
    "            input_date = input()\n",
    "            puzzle_url = 'https://news.naver.com/main/list.nhn?mode=LPOD&mid=sec&oid=' + get_number + '&date=' + input_date\n",
    "            print('\\n{}의 {}날짜 뉴스를 크롤링합니다.\\n'.format(input_press, input_date))\n",
    "\n",
    "        else:\n",
    "            print('\\n입력한 언론사가 리스트에 없습니다. https://news.naver.com/main/officeList.nhn 에 들어가서 원하는 언론사의 url을 \\\n",
    "입력해주세요.\\n')\n",
    "            puzzle_url = input()\n",
    "            self.get(puzzle_url)\n",
    "            input_now = self.page_source\n",
    "            input_source = BeautifulSoup(input_now, 'lxml')\n",
    "            page_list = input_source.find_all('div', {'class' : 'newsflash_header3'})\n",
    "            press_now = page_list[0].h3.text\n",
    "            self.press = press_now\n",
    "            print('\\n{} 맞나요? 원하는 날짜를 입력(yyyymmdd).\\n'.format(press_now))\n",
    "            test_date = input()\n",
    "            print('\\n{}의 {}날짜 뉴스를 크롤링합니다.\\n'.format(press_now, test_date))\n",
    "            puzzle_url = puzzle_url+ '&date=' + test_date\n",
    "\n",
    "        return puzzle_url\n",
    "    \n",
    "    def move_page(self, page_num): # 어떤 날의 여러 페이지 중에 하나로 이동하고 url 을 얻는 method\n",
    "        page_url = puzzle_url + '&page=' + str(page_num)\n",
    "        self.get(page_url)\n",
    "        return page_url\n",
    "    \n",
    "    def list_up(self, html):\n",
    "        \n",
    "        listup = BeautifulSoup(html, 'lxml')\n",
    "        lists = listup.find_all('a', {'class' : 'nclicks(cnt_papaerart)'})\n",
    "        lists += listup.find_all('a', {'class' : 'nclicks(cnt_papaerart3)'})\n",
    "        lists += listup.find_all('a', {'class' : 'nclicks(cnt_papaerart4)'})\n",
    "        lists += listup.find_all('a', {'class' : 'nclicks(cnt_flashart)'})\n",
    "        \n",
    "        news_list = [article for article in lists if type(article.find('img')) != bs4.element.Tag] # 이미지는 제외\n",
    "        \n",
    "        return news_list\n",
    "    \n",
    "    def break_check(self, news_list, list_tmp): #예시) 14페이지와 15페이지의 뉴스리스트가 같다면 break \n",
    "        \n",
    "        if(list_tmp == news_list[0]): \n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def exclude_sports_ent(self):\n",
    "        check = self.current_url\n",
    "        if ('sports' in check) or ('entertain' in check):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_data(self, speed = 0.2, num_comments = 700): # 제목, 분류, 날짜, 언론사, 내용, 댓글 수집 \n",
    "        # speed는 댓글 더보기를 누르는 간격, 0.1초로 하면 건너뛰는 경우가 있음. \n",
    "        # num_comments는 크롤링하고 싶은 댓글의 수, 삭제된 댓글 포함.\n",
    "        \n",
    "        html = self.page_source\n",
    "        dom = BeautifulSoup(html, 'lxml')\n",
    "        current_url = self.current_url\n",
    "\n",
    "        category_raw = dom.find('em', {'class' : 'guide_categorization_item'}) # 분류\n",
    "        category = category_raw.text\n",
    "\n",
    "        title_raw = dom.find_all('h3', {'id' : 'articleTitle'}) # 기사 제목\n",
    "        title = [title.text for title in title_raw]\n",
    "        title = str(title[0])\n",
    "        original_title = title # 제목 원본\n",
    "\n",
    "        title = re.sub('[^0-9a-zA-Zㄱ-힗]', '', title) # 저장시 문제 안생기게 전처리한 제목\n",
    "\n",
    "        date_raw = dom.find_all('span', {'class' : 't11'}) # 날짜\n",
    "        date = date_raw[0].text.split()[0]\n",
    "\n",
    "        press_raw = dom.find('div', {'class' : 'press_logo'}) #언론사\n",
    "        press = self.press\n",
    "\n",
    "        contents_raw = dom.find('div', {'id' : 'articleBodyContents'}) # 뉴스 내용\n",
    "        contents = contents_raw.text\n",
    "\n",
    "        # 네이버 뉴스에는 아래와 같은 주석이 항상 있음. 이 주석을 제거하기 위한 코드\n",
    "        # \\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\\n\\n \n",
    "        clean_index = contents.index('removeCallback') + 23\n",
    "        contents = contents[clean_index :]\n",
    "\n",
    "        # 기사 포맷이 거의 항상 아래와 같음. 필요 없는 정보를 제거하기 위한 코드\n",
    "        # [ⓒ한겨레신문 : 무단전재 및 재배포 금지]\n",
    "        if '재배포' in contents:\n",
    "            reporter_index = contents.index('재배포') - 15\n",
    "            contents = contents[:reporter_index]\n",
    "\n",
    "        time.sleep(speed)\n",
    "        \n",
    "        try:\n",
    "            self.find_element_by_css_selector(\".u_cbox_in_view_comment\").click() #댓글 보기 누르는 코드\n",
    "            time.sleep(speed)\n",
    "        except exceptions.ElementNotInteractableException as e:\n",
    "            pass\n",
    "        except exceptions.NoSuchElementException as e:\n",
    "            try:\n",
    "                new_addr = dom.find_all('div', {'class' : 'simplecmt_links'})\n",
    "                new_addr = new_addr[0].select('a')[0]['href']\n",
    "                self.get(new_addr)\n",
    "                time.sleep(speed)\n",
    "            except:\n",
    "                pass\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            self.find_element_by_css_selector(\".u_cbox_sort_label\").click() #공감순으로 보기 누르는 코드\n",
    "            time.sleep(speed)\n",
    "        except exceptions.NoSuchElementException as e:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            for i in range(num_comments//20):\n",
    "                self.find_element_by_css_selector(\".u_cbox_btn_more\").click() # 댓글 더보기 누르는 코드\n",
    "                time.sleep(speed)\n",
    "        except exceptions.ElementNotVisibleException as e: #댓글 페이지 끝\n",
    "            pass\n",
    "\n",
    "        except Exception as e: # 다른 예외 발생시 확인\n",
    "            pass\n",
    "\n",
    "        html = self.page_source # 댓글 크롤링 코드\n",
    "        dom = BeautifulSoup(html, 'lxml')\n",
    "        comments_raw = dom.find_all('span', {'class' : 'u_cbox_contents'})\n",
    "        comments = [comment.text for comment in comments_raw]\n",
    "\n",
    "        like_comments_raw = dom.find_all('em', {'class' : 'u_cbox_cnt_recomm'}) # 공감수\n",
    "        like_comments = [int(like.text) for like in like_comments_raw]\n",
    "\n",
    "        hate_comments_raw = dom.find_all('em', {'class' : 'u_cbox_cnt_unrecomm'}) # 비공감수\n",
    "        hate_comments = [int(hate.text) for hate in hate_comments_raw]\n",
    "        \n",
    "        if (len(comments)<1): #댓글이 없는 경우\n",
    "            comments = []\n",
    "            like_comments = []\n",
    "            hate_comments = []\n",
    "        \n",
    "        data_list = [category, title, original_title, date, press, contents, comments, like_comments, hate_comments, current_url]\n",
    "        \n",
    "        return data_list\n",
    "    \n",
    "    def save_file(self, data_list):\n",
    "        \n",
    "        file_name = './'+ data_list[4] + '/' + data_list[0]+ '_'  + data_list[4] + '_' + data_list[3] +'_'+ data_list[1] + '.json'\n",
    "        file_data = OrderedDict()\n",
    "        \n",
    "        file_data['url'] = data_list[9]\n",
    "        file_data['press'] = data_list[4]\n",
    "        file_data['date'] = data_list[3]\n",
    "        file_data['category'] = data_list[0]\n",
    "        file_data['title'] = data_list[2]\n",
    "        file_data['contents'] = data_list[5]\n",
    "        file_data['comment'] = data_list[6]\n",
    "        file_data['like'] = data_list[7]\n",
    "        file_data['dont_like'] = data_list[8]\n",
    "\n",
    "        directory = './' + data_list[4]\n",
    "\n",
    "        if os.path.exists(directory):\n",
    "            with open(file_name, 'w', encoding = 'utf-8') as make_file:\n",
    "                json.dump(file_data, make_file, ensure_ascii=False, indent='\\t')\n",
    "\n",
    "        else:\n",
    "            os.mkdir(directory)\n",
    "            with open(file_name, 'w', encoding = 'utf-8') as make_file:\n",
    "                json.dump(file_data,  make_file,ensure_ascii=False, indent='\\t')\n",
    "                \n",
    "    def crawl_pages(self, num_page, speed, num_comment): # 크롤링할 페이지 수를 선택, 하루에 약 10페이지 정도 기사가 올라옴\n",
    "                                                         #한 페이지에는 보통 20개의 기사가 있음.\n",
    "        count = 0 # 크롤링한 기사 수 체크용\n",
    "        list_tmp = [0] # 페이지 체크용\n",
    "        \n",
    "        for i in range(num_page):\n",
    "            page_url = self.move_page(i+1)\n",
    "            today_html = self.page_source\n",
    "            news_list = self.list_up(today_html)\n",
    "    \n",
    "            if self.break_check(news_list,list_tmp): #예시) 14페이지와 15페이지의 뉴스리스트가 같다면 break \n",
    "                break\n",
    "            else:\n",
    "                list_tmp = news_list[0]\n",
    "\n",
    "            for index in range(len(news_list)):\n",
    "                try:\n",
    "                    count += 1\n",
    "                    addr = news_list[index]['href']\n",
    "                    self.get(addr)\n",
    "                    # 스포츠 뉴스와 연예 뉴스는 제외 (형식도 다르고 목적과 맞지 않음.)\n",
    "                    if self.exclude_sports_ent():\n",
    "                        continue\n",
    "\n",
    "                    data_list = self.get_data(speed, num_comment)\n",
    "                    print(data_list[9])\n",
    "                    print(\"\\\"{}\\\" 본문과 댓글 {}개를 크롤링.\\n\".format(data_list[2], len(data_list[7])))\n",
    "                    self.save_file(data_list) # 데이터 저장\n",
    "\n",
    "                except:\n",
    "                    print(data_list[9])\n",
    "                    print(\"Error\\n\")\n",
    "                    pass\n",
    "            \n",
    "        return count\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = \"./chromedriver\"\n",
    "driver = crawler(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤링을 원하는 언론사를 입력.\n",
      "\n",
      "ex) 경향신문, 국민일보, 동아일보, 문화일보, 서울신문, 세계일보, 조선일보, 중앙일보, 한겨레, 한국일보, 이외의 언론사는 0 입력\n",
      "\n",
      "0\n",
      "\n",
      "입력한 언론사가 리스트에 없습니다. https://news.naver.com/main/officeList.nhn 에 들어가서 원하는 언론사의 url을 입력해주세요.\n",
      "\n",
      "https://news.naver.com/main/list.nhn?mode=LPOD&mid=sec&oid=003\n",
      "\n",
      "뉴시스 맞나요? 원하는 날짜를 입력(yyyymmdd).\n",
      "\n",
      "20190808\n",
      "\n",
      "뉴시스의 20190808날짜 뉴스를 크롤링합니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "puzzle_url = driver.get_input() # 크롤링하고 싶은 언론사와 날짜를 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009391893\n",
      "\"美 노동시장 건재…지난주 실업수당 신청 8000건 줄어\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009391892\n",
      "\"조국 법무·최기영 과기·홍미영 여가…文, 내일 '중폭 개각'(종합2보)\" 본문과 댓글 45개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009391891\n",
      "\"생후 6일 아기 기린 \" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009391890\n",
      "\"동방정교 수장 앞에서 겸손함이 묻어나는 우크라이나 대통령\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009391889\n",
      "\"주미대사에 '외교통' 이수혁 의원 내정…문정인 고사에 전격 발탁\" 본문과 댓글 33개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009391888\n",
      "\"8월이면 메카에서 매일 볼 수 있는 광경\" 본문과 댓글 2개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009391887\n",
      "\"日 매체, 韓정부 '석탄재 관리 강화' 주목…\"日 국민 감정 자극\"\" 본문과 댓글 57개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009391885\n",
      "\"우리가 증인이다 '끝까지 싸웁시다'\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009391884\n",
      "\"러시아, 로켓 엔진 폭발로 2명 사망…방사능 일시 증가\" 본문과 댓글 1개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009391883\n",
      "\"경북 아파트 단지 7시간 정전…1900여가구 주민 불편\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009391881\n",
      "\"김복동 필름 소사이어티 토크\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009391880\n",
      "\"김복동 필름 소사이어티 토크\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009391879\n",
      "\"김복동 필름 소사이어티 토크\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009391876\n",
      "\"일본 정부 경제보복 규탄 목소리 '확산'\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009391875\n",
      "\"출력 이상 보인 한울원전 6호기 정상 가동\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009391874\n",
      "\"강원도 고랭지 우수 농산물 판매 대전\" 본문과 댓글 1개를 크롤링.\n",
      "\n",
      "################number of articles: 20################\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    count = driver.crawl_pages(1,0.2,50) #parameter 순서\n",
    "                                         #크롤링할 페이지 수(한 페이지에는 약 20개 기사가 있음.)\n",
    "                                         #크롤링 속도 조절(0.1로 하면 에러나기도 함)\n",
    "                                         #크롤링할 댓글 수 (삭제된 댓글 포함, 약간의 오차 있을 수 있음.)\n",
    "    print('################ number of articles: {} ################'.format(count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
